# Qwen3-Reranker MCP Server Configuration
# Copy this file to .env and customize as needed

# =============================================================================
# Model Configuration
# =============================================================================

# HuggingFace model ID (default: Qwen/Qwen3-Reranker-8B)
# QWEN3_RERANKER_MODEL_ID=Qwen/Qwen3-Reranker-8B

# Device for inference: cpu, cuda, mps, or auto
# Use cpu for high-RAM systems (64GB+ recommended)
QWEN3_RERANKER_DEVICE=cpu

# Model precision: float32, float16, or bfloat16
# float32 is recommended for maximum quality (no quantization)
QWEN3_RERANKER_TORCH_DTYPE=float32

# =============================================================================
# Inference Settings
# =============================================================================

# Maximum sequence length for (query, document) pairs
# QWEN3_RERANKER_MAX_LENGTH=8192

# Batch size for processing multiple pairs
# Adjust based on available RAM (8 is good for 64GB+)
# QWEN3_RERANKER_BATCH_SIZE=8

# Default number of top results for reranking
# QWEN3_RERANKER_DEFAULT_TOP_K=10

# =============================================================================
# Server Settings
# =============================================================================

# Logging level: DEBUG, INFO, WARNING, ERROR
QWEN3_RERANKER_LOG_LEVEL=INFO

# =============================================================================
# Cache Settings
# =============================================================================

# Custom cache directory for model weights
# Defaults to HuggingFace cache (~/.cache/huggingface)
# QWEN3_RERANKER_CACHE_DIR=/path/to/cache
